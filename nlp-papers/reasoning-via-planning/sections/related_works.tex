
% \vspace{-5pt}
\section{Related Work}
% \vspace{-5pt}



\noindent \textbf{Reasoning with LLMs.}
LLM reasoning typically involves decomposing complex questions into sequential intermediate steps (a.k.a. chains) before producing the final answer, exemplified by Chain-of-Thought (CoT) prompting and its variants~\cite{wei2022chain, kojima2022large}. The basic CoT generates chains all at once and can induce additional errors as the step count increases. Self-Consistency~\cite{wang2022self} samples multiple chains to choose the best answer via majority voting. Least-to-most prompting~\cite{zhou2022least} reduces the question into simpler subquestions and answers them sequentially. Similar to our reward formulation, recent works have explored self-evaluation approaches to provide feedback for intermediate steps~\cite{welleck2022generating, Shinn2023ReflexionAA, paul2023refiner}. Aligned with our state formulation, \citet{li2022language} incorporate latent ``situations'' into LLMs, referring to the state of entities from the context. More relevantly, recent works have started to explore more complex structures guided by some search algorithms. For instance, CoRe~\cite{zhu2022solving} fine-tunes reasoning step generator and verifier for math word problems with MCTS for decoding. Concurrently to our work, \citet{yao2023tree} apply heuristic-based search, like depth-/breadth-first search, for better reasoning paths. However, none of the above methods formally introduce the world model and instantiates the reward and state into a unified framework. Compared with these search-guided methods, \ours is a more principled framework to combine world model and reward with advanced planning.



%, which is designed to solve more diverse tasks. Specifically, \ours repurposes an LLM to enable flexible definitions of rewards without fine-tuning (see Section~\ref{sec:reward}), and integrates powerful search algorithms beyond heuristic search ones (see Section~\ref{sec:mcts}).

% On the other hand, exploring all possible reasoning chains or feedback can lead to scalable issues as the total number of possible chains is intractable after several steps. To tackle such issues, recent works~\cite{xie2023decomposition, Yao2023TreeOT, jung2022maieutic} use tree search to more efficiently sample possible reasoning steps. Specifically, \cite{xie2023decomposition} shares a similar decomposition strategy but in addition, follows a tree-search procedure where the LM generates multiple candidate solutions at each step toward the final problem. These candidate solutions are obtained using stochastic beam search decoding, which samples from the distribution of possible outputs. The LM then evaluates these candidates with self-evaluation, based on the quality and coherence of the generated responses. Another work similar to ours, \cite{Yao2023TreeOT}, also proposes a tree-search framework for deliberate problem-solving. The tree is constructed by considering different reasoning paths. The LLM generates and evaluates candidate paths and selects the most confident ones for further reasoning. Their approach, however, uses either depth/breadth-first search for candidate reasoning steps, while our work leverages MCTS for high-quality reward and more efficient candidate generation. CoRe\cite{zhu2022solving} introduces a cooperative reasoning framework where multiple LMs together solve math word problems. A reward model is designed to generate feedback based on the correctness of LM-generated solutions. It adopts MCTS to leverage the reward and guide the LMs to explore multiple paths for solving math problems. While their reward comes from a trained verifier, our work directly uses LLM itself to generate reward, which suits better in the zero-shot setting.

\noindent \textbf{Planning with LLMs.}
Planning, a central ability in intelligent agents, involves generating a series of actions to achieve a specific goal~\cite{mccarthy1963situations, bylander1994computational}. Classical planning methods have been widely adopted in robots and embodied environments~\cite{camacho2013model, jiang2019task}. Recently, prompting LLMs to do planning directly has gained attention and shown potential~\cite{huang2022inner, singh2022progprompt, ding2023task}. Moreover, based on LLMs' powerful programming ability~\cite{lyu2023faithful, jojic2023gpt, liu2023llm+}, recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning algorithms, such as LLM+P~\cite{liu2023llm+}. However, code-based planning is constrained by its narrow domains and the environment, while \ours can handle open-domain problems, such as math and logical reasoning. More related works on \emph{world models and planning} are discussed in the Appendix~\ref{sec:related_planning}.

% \noindent \textbf{World Models and Planning.}
% Traditional reinforcement learning (RL) heavily relies on interaction with the environment (real world or simulators). To improve sample efficiency, some works leverage a \textbf{world model} that predicts state transition, and directly learn a policy within the world model \cite{ha2018recurrent, ha2018world}. With latent imagination in a world model, an RL agent can be trained to solve long-horizon tasks~\cite{hafner2019dream, hafner2020mastering}. Recent years have witnessed successful applications of \textbf{planning} algorithms in RL~\cite{sekar2020planning}, such as AlphaZero~\cite{silver2017mastering}, MuZero~\cite{schrittwieser2020mastering}. These algorithms are typically based on tree searches and are designed to effectively maintain the balance of exploration and exploitation. Compared with previous works, we use LLMs as world models and apply a planning algorithm to search for better reasoning paths, to solve various open-domain reasoning tasks.

%Daydreamer \cite{wu2023daydreamer}
% Dreamer \cite{hafner2019dream}
% Dreamer V2 \cite{hafner2020mastering}

%\cite{shyam2019model}
%Policy learning
%and DreamerV2~\cite{hafner2020mastering}
%~\cite{shyam2019model}. \cite{ha2018recurrent, hafner2019dream, wu2023daydreamer}


%Yet, existing world model-based planning is mostly applied to RL environments with well-defined states and actions. In contrast, to the best of our knowledge, \ours is the first general framework repurposing an LLM as the world model, leveraging it to tackle LLM reasoning problems. 


% Classical planning algorithms, despite their effectiveness, often fall short due to their computational cost, especially with large state-action spaces. Advanced approaches have hence leveraged world models~\cite{ha2018world, matsuo2022deep}, i.e., the internal representation of the environment, which guide the planning by estimating future states and rewards, thereby reducing the computational cost and making planning more tractable in complex environments. 




% With the rapid progress of LLMs, many works study planning with LLM in open-ended worlds which requires the ability of multi-step reasoning. While SayCan~\cite{ahn2022can} combines LLMs with affordances to generate feasible plans, DEPS~\cite{wang2023describe} leverages chain-of-thought reasoning with goal-conditioned reinforcement learning to tackle open-world Minecraft planning tasks. Another relevant work~\cite{lyu2023faithful} based on chain-of-thought reasoning focuses on incorporating intermediate reasoning steps into the LLM's decision-making process, leading to improved performance by considering the coherence and consistency of generated responses. It first translates the original problem into executable symbolic language programs which may lead to inconsistency between questions and answers, which are in the original format (e.g. natural language) and symbolic reasoning process. LLM+P~\cite{liu2023llm+} integrates a planning proficiency component into LLM to enable multi-step reasoning and planning. Similarly, the model requires translations between planning prompts and planning domain definition language (PDDL) and can only handle predefined environments. Our work, on the other hand, is able to deal with open-domain situations. 


% \noindent \textbf{Monte Carlo Tree Search (MCTS).}

% \paragraph{World Model}
% We refer to World Models the models which simulates the real world, which can be used to predict the outcome of certain actions.
% Previous works have introduces various world model. Some of them utilize