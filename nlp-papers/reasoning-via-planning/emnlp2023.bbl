\begin{thebibliography}{59}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Baddeley(1992)}]{baddeley1992working}
Alan Baddeley. 1992.
\newblock Working memory.
\newblock \emph{Science}, 255(5044):556--559.

\bibitem[{Briscoe(2011)}]{briscoe2011mental}
Robert~Eamon Briscoe. 2011.
\newblock Mental imagery and the varieties of amodal perception.
\newblock \emph{Pacific Philosophical Quarterly}, 92(2):153--173.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:1877--1901.

\bibitem[{Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg et~al.}]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al. 2023.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}.

\bibitem[{Bylander(1994)}]{bylander1994computational}
Tom Bylander. 1994.
\newblock The computational complexity of propositional strips planning.
\newblock \emph{Artificial Intelligence}, 69(1-2):165--204.

\bibitem[{Camacho and Alba(2013)}]{camacho2013model}
Eduardo~F Camacho and Carlos~Bordons Alba. 2013.
\newblock \emph{Model predictive control}.
\newblock Springer science \& business media.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano et~al.}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al. 2021.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}.

\bibitem[{Coulom(2007)}]{coulom2007efficient}
R{\'e}mi Coulom. 2007.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In \emph{Computers and Games: 5th International Conference, CG 2006,
  Turin, Italy, May 29-31, 2006. Revised Papers 5}, pages 72--83. Springer.

\bibitem[{Ding et~al.(2023)Ding, Zhang, Paxton, and Zhang}]{ding2023task}
Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. 2023.
\newblock Task and motion planning with large language models for object
  rearrangement.
\newblock \emph{arXiv preprint arXiv:2303.06247}.

\bibitem[{Gasparski and Orel(2014)}]{gasparski2014designology}
Wojciech~W Gasparski and Tufan Orel. 2014.
\newblock \emph{Designology: Studies on Planning for Action}, volume~1.
\newblock Transaction Publishers.

\bibitem[{Gentner and Stevens(2014)}]{gentner2014mental}
Dedre Gentner and Albert~L Stevens. 2014.
\newblock \emph{Mental models}.
\newblock Psychology Press.

\bibitem[{Ha and Schmidhuber(2018{\natexlab{a}})}]{ha2018recurrent}
David Ha and J{\"u}rgen Schmidhuber. 2018{\natexlab{a}}.
\newblock Recurrent world models facilitate policy evolution.
\newblock \emph{Advances in neural information processing systems}, 31.

\bibitem[{Ha and Schmidhuber(2018{\natexlab{b}})}]{ha2018world}
David Ha and J{\"u}rgen Schmidhuber. 2018{\natexlab{b}}.
\newblock World models.
\newblock \emph{arXiv preprint arXiv:1803.10122}.

\bibitem[{Hafner et~al.(2019)Hafner, Lillicrap, Ba, and
  Norouzi}]{hafner2019dream}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. 2019.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{arXiv preprint arXiv:1912.01603}.

\bibitem[{Hafner et~al.(2020)Hafner, Lillicrap, Norouzi, and
  Ba}]{hafner2020mastering}
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. 2020.
\newblock Mastering atari with discrete world models.
\newblock \emph{arXiv preprint arXiv:2010.02193}.

\bibitem[{Hao et~al.(2023{\natexlab{a}})Hao, Liu, Wang, and
  Hu}]{hao2023toolkengpt}
Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023{\natexlab{a}}.
\newblock Toolkengpt: Augmenting frozen language models with massive tools via
  tool embeddings.
\newblock \emph{Advances in neural information processing systems}, 36.

\bibitem[{Hao et~al.(2023{\natexlab{b}})Hao, Tan, Tang, Ni, Shao, Zhang, Xing,
  and Hu}]{hao2023bertnet}
Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe Zhang, Eric
  Xing, and Zhiting Hu. 2023{\natexlab{b}}.
\newblock Bertnet: Harvesting knowledge graphs with arbitrary relations from
  pretrained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pages 5000--5015.

\bibitem[{Ho et~al.(2021)Ho, Abel, Correa, Littman, Cohen, and
  Griffiths}]{ho2021control}
Mark~K Ho, David Abel, Carlos~G Correa, Michael~L Littman, Jonathan~D Cohen,
  and Thomas~L Griffiths. 2021.
\newblock Control of mental representations in human planning.
\newblock \emph{arXiv e-prints}, pages arXiv--2105.

\bibitem[{Huang and Chang(2022)}]{huang2022towards}
Jie Huang and Kevin Chen-Chuan Chang. 2022.
\newblock Towards reasoning in large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2212.10403}.

\bibitem[{Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng,
  Tompson, Mordatch, Chebotar et~al.}]{huang2022inner}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy
  Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et~al. 2022.
\newblock Inner monologue: Embodied reasoning through planning with language
  models.
\newblock \emph{arXiv preprint arXiv:2207.05608}.

\bibitem[{Huys et~al.(2012)Huys, Eshel, O'Nions, Sheridan, Dayan, and
  Roiser}]{huys2012bonsai}
Quentin~JM Huys, Neir Eshel, Elizabeth O'Nions, Luke Sheridan, Peter Dayan, and
  Jonathan~P Roiser. 2012.
\newblock Bonsai trees in your head: how the pavlovian system sculpts
  goal-directed choices by pruning decision trees.
\newblock \emph{PLoS computational biology}, 8(3):e1002410.

\bibitem[{Jiang et~al.(2019)Jiang, Zhang, Khandelwal, and
  Stone}]{jiang2019task}
Yu-qian Jiang, Shi-qi Zhang, Piyush Khandelwal, and Peter Stone. 2019.
\newblock Task planning in robotics: an empirical comparison of pddl-and
  asp-based systems.
\newblock \emph{Frontiers of Information Technology \& Electronic Engineering},
  20:363--373.

\bibitem[{Johnson-Laird(2010)}]{johnson2010mental}
Philip~N Johnson-Laird. 2010.
\newblock Mental models and human reasoning.
\newblock \emph{Proceedings of the National Academy of Sciences},
  107(43):18243--18250.

\bibitem[{Johnson-Laird(1983)}]{johnson1983mental}
Philip~Nicholas Johnson-Laird. 1983.
\newblock \emph{Mental models: Towards a cognitive science of language,
  inference, and consciousness}.
\newblock 6. Harvard University Press.

\bibitem[{Jojic et~al.(2023)Jojic, Wang, and Jojic}]{jojic2023gpt}
Ana Jojic, Zhen Wang, and Nebojsa Jojic. 2023.
\newblock Gpt is becoming a turing machine: Here are some ways to program it.
\newblock \emph{arXiv preprint arXiv:2303.14310}.

\bibitem[{Kocsis and Szepesv{\'a}ri(2006)}]{kocsis2006bandit}
Levente Kocsis and Csaba Szepesv{\'a}ri. 2006.
\newblock Bandit based monte-carlo planning.
\newblock In \emph{Machine Learning: ECML 2006: 17th European Conference on
  Machine Learning Berlin, Germany, September 18-22, 2006 Proceedings 17},
  pages 282--293. Springer.

\bibitem[{Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa}]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa. 2022.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{arXiv preprint arXiv:2205.11916}.

\bibitem[{LeCun(2022)}]{lecun2022path}
Yann LeCun. 2022.
\newblock A path towards autonomous machine intelligence version 0.9. 2,
  2022-06-27.
\newblock \emph{Open Review}, 62.

\bibitem[{Li et~al.(2022)Li, Nye, and Andreas}]{li2022language}
Belinda~Z Li, Maxwell Nye, and Jacob Andreas. 2022.
\newblock Language modeling with latent situations.
\newblock \emph{arXiv preprint arXiv:2212.10012}.

\bibitem[{Liu et~al.(2023)Liu, Jiang, Zhang, Liu, Zhang, Biswas, and
  Stone}]{liu2023llm+}
Bo~Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas,
  and Peter Stone. 2023.
\newblock Llm+ p: Empowering large language models with optimal planning
  proficiency.
\newblock \emph{arXiv preprint arXiv:2304.11477}.

\bibitem[{Lyu et~al.(2023)Lyu, Havaldar, Stein, Zhang, Rao, Wong, Apidianaki,
  and Callison-Burch}]{lyu2023faithful}
Qing Lyu, Shreya Havaldar, Adam Stein, Li~Zhang, Delip Rao, Eric Wong, Marianna
  Apidianaki, and Chris Callison-Burch. 2023.
\newblock Faithful chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2301.13379}.

\bibitem[{Matsuo et~al.(2022)Matsuo, LeCun, Sahani, Precup, Silver, Sugiyama,
  Uchibe, and Morimoto}]{matsuo2022deep}
Yutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina Precup, David Silver, Masashi
  Sugiyama, Eiji Uchibe, and Jun Morimoto. 2022.
\newblock Deep learning, reinforcement learning, and world models.
\newblock \emph{Neural Networks}.

\bibitem[{McCarthy(1963)}]{mccarthy1963situations}
John McCarthy. 1963.
\newblock Situations, actions, and causal laws.
\newblock Technical report, STANFORD UNIV CA DEPT OF COMPUTER SCIENCE.

\bibitem[{Mialon et~al.(2023)Mialon, Dess{\`\i}, Lomeli, Nalmpantis, Pasunuru,
  Raileanu, Rozi{\`e}re, Schick, Dwivedi-Yu, Celikyilmaz
  et~al.}]{mialon2023augmented}
Gr{\'e}goire Mialon, Roberto Dess{\`\i}, Maria Lomeli, Christoforos Nalmpantis,
  Ram Pasunuru, Roberta Raileanu, Baptiste Rozi{\`e}re, Timo Schick, Jane
  Dwivedi-Yu, Asli Celikyilmaz, et~al. 2023.
\newblock Augmented language models: a survey.
\newblock \emph{arXiv preprint arXiv:2302.07842}.

\bibitem[{OpenAI(2023)}]{openai2023gpt4}
OpenAI. 2023.
\newblock \href {http://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.

\bibitem[{Paul et~al.(2023)Paul, Ismayilzada, Peyrard, Borges, Bosselut, West,
  and Faltings}]{paul2023refiner}
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
  Bosselut, Robert West, and Boi Faltings. 2023.
\newblock Refiner: Reasoning feedback on intermediate representations.
\newblock \emph{arXiv preprint arXiv:2304.01904}.

\bibitem[{Puig et~al.(2018)Puig, Ra, Boben, Li, Wang, Fidler, and
  Torralba}]{puig2018virtualhome}
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and
  Antonio Torralba. 2018.
\newblock \href {http://arxiv.org/abs/1806.07011} {Virtualhome: Simulating
  household activities via programs}.

\bibitem[{Saparov and He(2022)}]{saparov2022language}
Abulhair Saparov and He~He. 2022.
\newblock Language models are greedy reasoners: A systematic formal analysis of
  chain-of-thought.
\newblock \emph{arXiv preprint arXiv:2210.01240}.

\bibitem[{Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`\i}, Raileanu, Lomeli,
  Zettlemoyer, Cancedda, and Scialom}]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria
  Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}.

\bibitem[{Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert,
  Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel
  et~al.}]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al. 2020.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588(7839):604--609.

\bibitem[{Schulkin(2012)}]{schulkin2012action}
Jay Schulkin. 2012.
\newblock \emph{Action, perception and the brain: Adaptation and cephalic
  expression}.
\newblock Springer.

\bibitem[{Sekar et~al.(2020)Sekar, Rybkin, Daniilidis, Abbeel, Hafner, and
  Pathak}]{sekar2020planning}
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner,
  and Deepak Pathak. 2020.
\newblock Planning to explore via self-supervised world models.
\newblock In \emph{International Conference on Machine Learning}, pages
  8583--8592. PMLR.

\bibitem[{Shinn et~al.(2023)Shinn, Labash, and Gopinath}]{Shinn2023ReflexionAA}
Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023.
\newblock Reflexion: an autonomous agent with dynamic memory and
  self-reflection.
\newblock \emph{ArXiv}, abs/2303.11366.

\bibitem[{Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel et~al.}]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al. 2017.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}.

\bibitem[{Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,
  Thomason, and Garg}]{singh2022progprompt}
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan
  Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2022.
\newblock Progprompt: Generating situated robot task plans using large language
  models.
\newblock \emph{arXiv preprint arXiv:2209.11302}.

\bibitem[{Tolman(1948)}]{tolman1948cognitive}
Edward~C Tolman. 1948.
\newblock Cognitive maps in rats and men.
\newblock \emph{Psychological review}, 55(4):189.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar
  et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale
  et~al.}]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al. 2023{\natexlab{b}}.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and
  Kambhampati}]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
  2022.
\newblock Large language models still can't plan (a benchmark for llms on
  planning and reasoning about change).
\newblock \emph{arXiv preprint arXiv:2206.10498}.

\bibitem[{Valmeekam et~al.(2023)Valmeekam, Sreedharan, Marquez, Olmo, and
  Kambhampati}]{valmeekam2023planning}
Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and
  Subbarao Kambhampati. 2023.
\newblock On the planning abilities of large language models (a critical
  investigation with a proposed benchmark).
\newblock \emph{arXiv preprint arXiv:2302.06706}.

\bibitem[{Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and
  Zhou}]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, and Denny Zhou. 2022.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou. 2022.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}.

\bibitem[{Welleck et~al.(2022)Welleck, Lu, West, Brahman, Shen, Khashabi, and
  Choi}]{welleck2022generating}
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel
  Khashabi, and Yejin Choi. 2022.
\newblock Generating sequences by learning to self-correct.
\newblock \emph{arXiv preprint arXiv:2211.00053}.

\bibitem[{Wu et~al.(2023)Wu, Escontrela, Hafner, Abbeel, and
  Goldberg}]{wu2023daydreamer}
Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken
  Goldberg. 2023.
\newblock Daydreamer: World models for physical robot learning.
\newblock In \emph{Conference on Robot Learning}, pages 2226--2240. PMLR.

\bibitem[{Xiang et~al.(2023)Xiang, Tao, Gu, Shu, Wang, Yang, and
  Hu}]{xiang2023language}
Jiannan Xiang, Tianhua Tao, Yi~Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and
  Zhiting Hu. 2023.
\newblock Language models meet world models: Embodied experiences enhance
  language models.
\newblock \emph{Advances in neural information processing systems}, 36.

\bibitem[{Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan}]{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao,
  and Karthik Narasimhan. 2023.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{arXiv preprint arXiv:2305.10601}.

\bibitem[{Zhou et~al.(2022)Zhou, Sch{\"a}rli, Hou, Wei, Scales, Wang,
  Schuurmans, Bousquet, Le, and Chi}]{zhou2022least}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi
  Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed~Chi. 2022.
\newblock Least-to-most prompting enables complex reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2205.10625}.

\bibitem[{Zhu et~al.(2022)Zhu, Wang, Zhang, Zhang, Gan, Zhang, and
  Yang}]{zhu2022solving}
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and
  Yujiu Yang. 2022.
\newblock Solving math word problem via cooperative reasoning induced language
  models.
\newblock \emph{arXiv preprint arXiv:2210.16257}.

\end{thebibliography}
